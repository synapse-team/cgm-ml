{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PointNet (https://arxiv.org/abs/1612.00593).\n",
    "\n",
    "This notebook shows you how to use the PreprocessedDataGenerator in order to train PointNet.\n",
    "\n",
    "The PreprocessedDataGenerator uses preprocessed-data instead of ETL-data. Wheras ETL-data comes mainly as PCD-files, preprocessed-data comes mainly as pointclouds stored as numpy-arrays. We identified PCD-loading as a bottleneck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import talos as ta\n",
    "from cgmcore.preprocesseddatagenerator import get_dataset_path\n",
    "from cgmcore.preprocesseddatagenerator import create_datagenerator_from_parameters\n",
    "\n",
    "from cgmcore import modelutils\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 20\n",
    "validation_steps = 10\n",
    "dataset_path = \"../../preprocessed_trimmed/2018_07_31_10_52\"\n",
    "\n",
    "print(\"Using dataset path\", dataset_path)\n",
    "random_seed = 300\n",
    "\n",
    "dataset_parameters_pointclouds = {}\n",
    "dataset_parameters_pointclouds[\"input_type\"] = \"pointcloud\"\n",
    "dataset_parameters_pointclouds[\"output_targets\"] = [\"height\"]\n",
    "dataset_parameters_pointclouds[\"random_seed\"] = random_seed\n",
    "dataset_parameters_pointclouds[\"pointcloud_target_size\"] = 10000\n",
    "dataset_parameters_pointclouds[\"pointcloud_random_rotation\"] = False\n",
    "dataset_parameters_pointclouds[\"sequence_length\"] = 0\n",
    "datagenerator_instance_pointclouds = create_datagenerator_from_parameters(dataset_path, dataset_parameters_pointclouds)\n",
    "\n",
    "# Get the QR-codes.\n",
    "qrcodes_to_use = datagenerator_instance_pointclouds.qrcodes[0:1500]\n",
    "\n",
    "# Do the split.\n",
    "random.seed(random_seed)\n",
    "qrcodes_shuffle = qrcodes_to_use[:]\n",
    "random.shuffle(qrcodes_shuffle)\n",
    "split_index_a = int(0.8 * len(qrcodes_shuffle))\n",
    "#split_index_b = int(0.8 * 0.8 * len(qrcodes_shuffle))\n",
    "qrcodes_train = sorted(qrcodes_shuffle[:split_index_a])\n",
    "#qrcodes_validate = sorted(qrcodes_shuffle[split_index_b:split_index_a])\n",
    "qrcodes_test = sorted(qrcodes_shuffle[split_index_a:])\n",
    "del qrcodes_shuffle\n",
    "#print(\"QR-codes for training:\\n\", \"\\t\".join(qrcodes_train))\n",
    "#print(\"QR-codes for validation:\\n\", \"\\t\".join(qrcodes_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "\n",
    "    \n",
    "    # Create python generators.\n",
    "    generator_pointclouds_train = datagenerator_instance_pointclouds.generate(size=50,qrcodes_to_use=qrcodes_train)\n",
    "    generator_pointclouds_test = datagenerator_instance_pointclouds.generate(size=50,qrcodes_to_use=qrcodes_test)\n",
    "    # generator_pointclouds_validate = datagenerator_instance_pointclouds.generate(size=batch_size, qrcodes_to_use=qrcodes_validate)\n",
    "    \n",
    "    size = 16000\n",
    "    generator_pointclouds = generator_pointclouds_train\n",
    "    X = []\n",
    "    Y = []\n",
    "    d = next(generator_pointclouds)\n",
    "    while d:\n",
    "        t_x, t_y = d\n",
    "        for x in t_x:\n",
    "            X.append(x)\n",
    "        for y in t_y:\n",
    "            Y.append(y)\n",
    "        d = next(generator_pointclouds)\n",
    "        if len(X) > size:\n",
    "            break\n",
    "\n",
    "    train_x = X\n",
    "    train_y = Y\n",
    "\n",
    "    size = 4000\n",
    "    generator_pointclouds = generator_pointclouds_test\n",
    "    X = []\n",
    "    Y = []\n",
    "    d = next(generator_pointclouds)\n",
    "    while d:\n",
    "        t_x, t_y = d\n",
    "        for x in t_x:\n",
    "            X.append(x)\n",
    "        for y in t_y:\n",
    "            Y.append(y)\n",
    "        d = next(generator_pointclouds)\n",
    "        if len(X) > size:\n",
    "            break\n",
    "\n",
    "    test_x = X\n",
    "    test_y = Y\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training PointNet.\n",
    "\n",
    "The module modelutils contains methods for creating Neural Nets. The following code shows how to instantiate and train PointNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global input_shape \n",
    "input_shape = (dataset_parameters_pointclouds[\"pointcloud_target_size\"], 3)\n",
    "global output_size\n",
    "output_size = 1\n",
    "global hidden_sizes \n",
    "hidden_sizes = [64]\n",
    "\n",
    "def create_point_net(train_x, train_y, test_x, test_y):\n",
    "    \"\"\"\n",
    "    Creates a PointNet.\n",
    "\n",
    "    See https://github.com/garyloveavocado/pointnet-keras/blob/master/train_cls.py\n",
    "\n",
    "    Args:\n",
    "        input_shape (shape): Input-shape.\n",
    "        output_size (int): Output-size.\n",
    "\n",
    "    Returns:\n",
    "        Model: A model.\n",
    "    \"\"\"\n",
    "    \n",
    "    training_details = {\n",
    "    \"dataset_path\" : dataset_path,\n",
    "    \"qrcodes_train\" : qrcodes_train,\n",
    "    \"qrcodes_test\" : qrcodes_test,\n",
    "    \"steps_per_epoch\" : steps_per_epoch,\n",
    "    \"validation_steps\" : validation_steps,\n",
    "    \"epochs\" : params['epochs'],\n",
    "    \"batch_size\" : params['batch_size'],\n",
    "    \"random_seed\" : random_seed,\n",
    "    }\n",
    "\n",
    "    num_points = input_shape[0]\n",
    "\n",
    "    def mat_mul(A, B):\n",
    "        result = tf.matmul(A, B)\n",
    "        return result\n",
    "\n",
    "    input_points = layers.Input(shape=input_shape)\n",
    "    x = layers.Convolution1D(64, 1, activation='relu',\n",
    "                      input_shape=input_shape)(input_points)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Convolution1D(128, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Convolution1D(1024, 1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=num_points)(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(9, weights=[np.zeros([256, 9]), np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32)])(x)\n",
    "    input_T = layers.Reshape((3, 3))(x)\n",
    "\n",
    "    # forward net\n",
    "    g = layers.Lambda(mat_mul, arguments={'B': input_T})(input_points)\n",
    "    g = layers.Convolution1D(64, 1, input_shape=input_shape, activation='relu')(g)\n",
    "    g = layers.BatchNormalization()(g)\n",
    "    g = layers.Convolution1D(64, 1, input_shape=input_shape, activation='relu')(g)\n",
    "    g = layers.BatchNormalization()(g)\n",
    "\n",
    "    # feature transform net\n",
    "    f = layers.Convolution1D(64, 1, activation='relu')(g)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.Convolution1D(128, 1, activation='relu')(f)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.Convolution1D(1024, 1, activation='relu')(f)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.MaxPooling1D(pool_size=num_points)(f)\n",
    "    f = layers.Dense(512, activation='relu')(f)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.Dense(256, activation='relu')(f)\n",
    "    f = layers.BatchNormalization()(f)\n",
    "    f = layers.Dense(64 * 64, weights=[np.zeros([256, 64 * 64]), np.eye(64).flatten().astype(np.float32)])(f)\n",
    "    feature_T = layers.Reshape((64, 64))(f)\n",
    "\n",
    "    # forward net\n",
    "    g = layers.Lambda(mat_mul, arguments={'B': feature_T})(g)\n",
    "    g = layers.Convolution1D(64, 1, activation='relu')(g)\n",
    "    g = layers.BatchNormalization()(g)\n",
    "    g = layers.Convolution1D(128, 1, activation='relu')(g)\n",
    "    g = layers.BatchNormalization()(g)\n",
    "    g = layers.Convolution1D(1024, 1, activation='relu')(g)\n",
    "    g = layers.BatchNormalization()(g)\n",
    "\n",
    "    # global_feature\n",
    "    global_feature = layers.MaxPooling1D(pool_size=num_points)(g)\n",
    "\n",
    "    # point_net_cls\n",
    "    c = global_feature\n",
    "    for hidden_size in hidden_sizes:\n",
    "        c = layers.Dense(hidden_size, activation='relu')(c)\n",
    "        c = layers.BatchNormalization()(c)\n",
    "        c = layers.Dropout(rate=0.7)(c)\n",
    "    \n",
    "    c = layers.Dense(output_size, activation='linear')(c)\n",
    "    prediction = layers.Flatten()(c)\n",
    "\n",
    "    model = models.Model(inputs=input_points, outputs=prediction)\n",
    "    \n",
    "    sgd = optimizers.SGD(lr=0.0001,decay=1e-6,momentum=0.5,nesterov=True)\n",
    "    \n",
    "    model.compile(optimizer=sgd, loss=\"mse\", metrics=[\"mae\"])\n",
    "    \n",
    "    result = model.fit(\n",
    "        train_x,\n",
    "        batch_size=params['batch_size'],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=params['epochs'],\n",
    "        validation_split=0.2\n",
    "        )\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    X_train, Y_train, X_test, Y_test = data()\n",
    "    \n",
    "    p = {\n",
    "        'batch_size': [2, 3, 4],\n",
    "        'epochs': [10]\n",
    "        }\n",
    "    \n",
    "    h = ta.Scan(X_train, Y_train,\n",
    "          params=p,\n",
    "          dataset_name='first_test',\n",
    "          experiment_no='1',\n",
    "          model=create_model,\n",
    "          grid_downsample=0.5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving everything.\n",
    "\n",
    "This saves the model, its history and the training-details to some output directory. The created artifacts can later be uses in order to compare different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \".\"\n",
    "\n",
    "modelutils.save_model_and_history(output_path, model_pointnet, history, training_details, \"pointnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
